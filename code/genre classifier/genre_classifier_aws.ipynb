{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nimport pandas as pd\nimport re\nfrom itertools import chain\nimport joblib\nimport boto3\nimport pickle\nimport io\nfrom io import BytesIO\n\n# Download NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: 26614f9d-3e06-4f8a-a3e0-1e7cf90771aa\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session 26614f9d-3e06-4f8a-a3e0-1e7cf90771aa to get into ready status...\nSession 26614f9d-3e06-4f8a-a3e0-1e7cf90771aa has been created.\nTrue\n[nltk_data] Downloading package punkt to /home/spark/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package stopwords to /home/spark/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package wordnet to /home/spark/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /home/spark/nltk_data...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "stop_words = set(stopwords.words('english'))\nwnet = nltk.WordNetLemmatizer()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def read_txt_file_from_s3(bucket, key):\n    try:\n        # Recupera o objeto do S3\n        obj = s3_client.get_object(Bucket=bucket, Key=key)\n        # Lê o conteúdo do arquivo\n        content = obj['Body'].read().decode('utf-8')\n        return content\n    except Exception as e:\n        print(f\"Erro ao ler o arquivo do S3: {str(e)}\")\n        return None",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "genres = [\n    'Country',\n    'Rap',\n    'Rock'\n]",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3_bucket = 'ml-models-sprint2'\nvectorize_key = 'vectorize.pkl'\nrap_key = 'Rap_model.pkl'\nrock_key = 'Rock_model.pkl'\ncountry_key = 'Country_model.pkl'\nloaded_models = {}\ns3_client = boto3.client('s3')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "for genre in genres:\n    model_key = f'{genre}_model.pkl'\n    local_model_path = f'/tmp/{model_key}'\n    s3_client.download_file(s3_bucket, model_key, local_model_path)\n    with open(local_model_path, 'rb') as f:\n        loaded_models[genre] = pickle.load(f)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "/home/spark/.local/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MultinomialNB from version 1.4.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Caminho temporário local no Glue para salvar o modelo baixado\nlocal_model_path = '/tmp/rap.pkl'\n\n# Baixar o modelo do S3\ns3_client.download_file(s3_bucket, vectorize_key, local_model_path)\n\n# Carregar o modelo utilizando joblib (ou pickle)\nwith open(\"/tmp/rap.pkl\", \"rb\") as f:\n    Rap_model = pickle.load(f)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Caminho temporário local no Glue para salvar o modelo baixado\nlocal_model_path = '/tmp/rock.pkl'\n\n# Baixar o modelo do S3\ns3_client.download_file(s3_bucket, vectorize_key, local_model_path)\n\n# Carregar o modelo utilizando joblib (ou pickle)\nwith open(\"/tmp/rock.pkl\", \"rb\") as f:\n    Rock_model = pickle.load(f)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Caminho temporário local no Glue para salvar o modelo baixado\nlocal_model_path = '/tmp/country.pkl'\n\n# Baixar o modelo do S3\ns3_client.download_file(s3_bucket, vectorize_key, local_model_path)\n\n# Carregar o modelo utilizando joblib (ou pickle)\nwith open(\"/tmp/country.pkl\", \"rb\") as f:\n    Country = pickle.load(f)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Caminho temporário local no Glue para salvar o modelo baixado\nlocal_model_path = '/tmp/vectorize.pkl'\n\n# Baixar o modelo do S3\ns3_client.download_file(s3_bucket, vectorize_key, local_model_path)\n\n# Carregar o modelo utilizando joblib (ou pickle)\nwith open(\"/tmp/vectorize.pkl\", \"rb\") as f:\n    vectorize = pickle.load(f)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "file_path = 'lyric.txt'\ntext_content = read_txt_file_from_s3(s3_bucket, file_path)\n\nif text_content:\n    df = pd.DataFrame({'lyrics': [text_content]})\n    print(df.head())\nelse:\n    print(\"Falha ao ler o conteúdo do arquivo do S3.\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "                                              lyrics\n0  I used to spend my nights out in a barroom\\r\\n...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "text_content = read_txt_file_from_s3(s3_bucket, file_path)\ndf = pd.DataFrame({'lyrics': [text_content]})\ntokenized = [word_tokenize(lyr) for lyr in df['lyrics'].astype(str)]\nstop_vec = [[w for w in tok if w not in stop_words] for tok in tokenized]\nclean_vec = [[word for word in lyr if word.isalpha()] for lyr in stop_vec]\nlem = [[wnet.lemmatize(w) for w in lyr] for lyr in clean_vec]\nlem",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "[['I', 'used', 'spend', 'night', 'barroom', 'Liquor', 'love', 'I', 'known', 'But', 'rescued', 'reachin', 'bottom', 'And', 'brought', 'back', 'far', 'gone', 'You', 'smooth', 'Tennessee', 'whiskey', 'You', 'sweet', 'strawberry', 'wine', 'You', 'warm', 'glass', 'brandy', 'And', 'honey', 'I', 'stay', 'stoned', 'love', 'time', 'I', 'looked', 'love', 'old', 'place', 'Found', 'bottom', 'bottle', 'always', 'dry', 'But', 'poured', 'heart', 'I', 'waste', 'Cause', 'nothing', 'like', 'love', 'get', 'high', 'You', 'smooth', 'Tennessee', 'whiskey', 'You', 'sweet', 'strawberry', 'wine', 'You', 'warm', 'glass', 'brandy', 'And', 'honey', 'I', 'stay', 'stoned', 'love', 'time', 'You', 'smooth', 'Tennessee', 'whiskey', 'You', 'sweet', 'strawberry', 'wine', 'You', 'warm', 'glass', 'brandy', 'And', 'honey', 'I', 'stay', 'stoned', 'love', 'time', 'You', 'smooth', 'Tennessee', 'whiskey', 'Tennessee', 'whiskey', 'Tennessee', 'whiskey', 'You', 'smooth', 'Tennessee', 'whiskey', 'Tennessee', 'whiskey', 'Tennessee', 'whiskey']]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "lyrics_tay = [' '.join(lyr) for lyr in lem]\nsingle_entry = vectorize.transform(lyrics_tay)\ns_e = single_entry.todense().tolist()\n\nprobabilities = {genre: loaded_models[genre].predict_proba(s_e)[0][1] for genre in genres}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "/home/spark/.local/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n  warnings.warn(\n/home/spark/.local/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n  warnings.warn(\n/home/spark/.local/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n  warnings.warn(\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "for genre, prob in probabilities.items():\n    print(f\"{genre}: {prob * 100:.2f}%\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 55,
			"outputs": [
				{
					"name": "stdout",
					"text": "Country: 80.87%\nRap: 10.04%\nRock: 47.82%\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}