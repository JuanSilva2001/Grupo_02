{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joao.victor.ribeiro\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joao.victor.ribeiro\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import chain\n",
    "import joblib\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = pd.read_csv('artists-data.csv')\n",
    "songs = pd.read_csv('lyrics-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_list = [str(s).split(';') for s in artists['Genres'].unique()]\n",
    "res = list(chain(*genres_list))\n",
    "res = [gen.strip() for gen in res]\n",
    "genres = list(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in genres:\n",
    "    contains = [True if re.search(genre, str(art_gen)) else False for art_gen in artists['Genres']]\n",
    "    artists[genre] = contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_songs = songs.merge(artists, how='outer', left_on='ALink', right_on='Link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_country = all_songs[all_songs['Country']==False].sample(n=2500, random_state=1)\n",
    "yes_country = all_songs[all_songs['Country']==True].sample(n=2500, random_state=1)\n",
    "no_rock = all_songs[all_songs['Rock']==False].sample(n=2500, random_state=1)\n",
    "yes_rock = all_songs[all_songs['Rock']==True].sample(n=2500, random_state=1)\n",
    "no_rap = all_songs[all_songs['Rap']==False].sample(n=2500, random_state=1)\n",
    "yes_rap = all_songs[all_songs['Rap']==True].sample(n=2500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_songs = pd.concat([no_country, yes_country, no_rock, yes_rock, no_rap, yes_rap]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = all_songs['Lyric'].astype(str)\n",
    "low = [lyr.lower() for lyr in lyrics]\n",
    "tokenized = [word_tokenize(lyr) for lyr in low]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_vec = [[w for w in tok if w not in stop_words] for tok in tokenized]\n",
    "clean_vec = [[word for word in lyr if word.isalpha()] for lyr in stop_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = [' '.join(lyr) for lyr in clean_vec]\n",
    "vectorize = TfidfVectorizer(min_df=5, max_df=0.8)\n",
    "vectors = vectorize.fit_transform(lyrics)\n",
    "feature_names = vectorize.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "dense_list = dense.tolist()\n",
    "df = pd.DataFrame(dense_list, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'] = all_songs['Country'].copy()\n",
    "df['Rock'] = all_songs['Rock'].copy()\n",
    "df['Rap'] = all_songs['Rap'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Country', 'Rock', 'Rap']\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Country:\n",
      "Accuracy: 0.8253333333333334\n",
      "Precision: 0.6262626262626263\n",
      "Recall: 0.21602787456445993\n",
      "Confusion Matrix: \n",
      "[[3528  111]\n",
      " [ 675  186]]\n",
      "\n",
      "Results for Rock:\n",
      "Accuracy: 0.7206666666666667\n",
      "Precision: 0.5460434983803795\n",
      "Recall: 0.8104395604395604\n",
      "Confusion Matrix: \n",
      "[[2063  981]\n",
      " [ 276 1180]]\n",
      "\n",
      "Results for Rap:\n",
      "Accuracy: 0.9042222222222223\n",
      "Precision: 0.7940761636107193\n",
      "Recall: 0.6639150943396226\n",
      "Confusion Matrix: \n",
      "[[3506  146]\n",
      " [ 285  563]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for genre in genres:\n",
    "    y = df[genre].copy()\n",
    "    x = df.drop(genres, axis=1)  # Remove all genre columns\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)\n",
    "    \n",
    "    clf = MultinomialNB(alpha=0.1)\n",
    "    clf.fit(x_train, y_train.astype(bool))\n",
    "    models[genre] = clf\n",
    "    \n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test.astype(bool), y_pred)\n",
    "    precision = metrics.precision_score(y_test.astype(bool), y_pred)\n",
    "    recall = metrics.recall_score(y_test.astype(bool), y_pred)\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test.astype(bool), y_pred)\n",
    "    \n",
    "    print(f\"Results for {genre}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uh, uh (uh, come on)\\nHa, sicker than yo' aver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics\n",
       "0  Uh, uh (uh, come on)\\nHa, sicker than yo' aver..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'lyric.txt'\n",
    "text_content = read_txt_file(file_path)\n",
    "df = pd.DataFrame({'lyrics': [text_content]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'lyric.csv'\n",
    "\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Uh',\n",
       "  'uh',\n",
       "  'uh',\n",
       "  'come',\n",
       "  'Ha',\n",
       "  'sicker',\n",
       "  'yo',\n",
       "  'average',\n",
       "  'Poppa',\n",
       "  'twist',\n",
       "  'cabbage',\n",
       "  'instinct',\n",
       "  'Niggas',\n",
       "  'think',\n",
       "  'shit',\n",
       "  'stink',\n",
       "  'Pink',\n",
       "  'gator',\n",
       "  'Detroit',\n",
       "  'player',\n",
       "  'Timbs',\n",
       "  'hooligan',\n",
       "  'Brooklyn',\n",
       "  'right',\n",
       "  'Dead',\n",
       "  'right',\n",
       "  'head',\n",
       "  'right',\n",
       "  'Biggie',\n",
       "  'night',\n",
       "  'Poppa',\n",
       "  'smooth',\n",
       "  'since',\n",
       "  'day',\n",
       "  'Underoos',\n",
       "  'Never',\n",
       "  'lose',\n",
       "  'never',\n",
       "  'choose',\n",
       "  'bruise',\n",
       "  'crew',\n",
       "  'Do',\n",
       "  'somethin',\n",
       "  'u',\n",
       "  'talk',\n",
       "  'go',\n",
       "  'u',\n",
       "  'come',\n",
       "  'Girls',\n",
       "  'walk',\n",
       "  'u',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'u',\n",
       "  'screw',\n",
       "  'u',\n",
       "  'Who',\n",
       "  'u',\n",
       "  'Yeah',\n",
       "  'Poppa',\n",
       "  'Puff',\n",
       "  'Close',\n",
       "  'like',\n",
       "  'Starsky',\n",
       "  'Hutch',\n",
       "  'stick',\n",
       "  'clutch',\n",
       "  'Dare',\n",
       "  'I',\n",
       "  'squeeze',\n",
       "  'three',\n",
       "  'yo',\n",
       "  'cherry',\n",
       "  'take',\n",
       "  'take',\n",
       "  'take',\n",
       "  'ha',\n",
       "  'ha',\n",
       "  'Bang',\n",
       "  'every',\n",
       "  'MC',\n",
       "  'easily',\n",
       "  'busily',\n",
       "  'Recently',\n",
       "  'nigga',\n",
       "  'frontin',\n",
       "  'ai',\n",
       "  'sayin',\n",
       "  'nothin',\n",
       "  'nothin',\n",
       "  'So',\n",
       "  'I',\n",
       "  'speak',\n",
       "  'piece',\n",
       "  'keep',\n",
       "  'peace',\n",
       "  'come',\n",
       "  'Cubans',\n",
       "  'Jesus',\n",
       "  'piece',\n",
       "  'peep',\n",
       "  'thank',\n",
       "  'Packin',\n",
       "  'askin',\n",
       "  'Who',\n",
       "  'want',\n",
       "  'You',\n",
       "  'got',\n",
       "  'nigga',\n",
       "  'flaunt',\n",
       "  'That',\n",
       "  'Brooklyn',\n",
       "  'bullshit',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'ca',\n",
       "  'see',\n",
       "  'Sometimes',\n",
       "  'word',\n",
       "  'hypnotize',\n",
       "  'And',\n",
       "  'I',\n",
       "  'love',\n",
       "  'flashy',\n",
       "  'way',\n",
       "  'uh',\n",
       "  'Guess',\n",
       "  'broke',\n",
       "  'paid',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'ca',\n",
       "  'see',\n",
       "  'Uh',\n",
       "  'Sometimes',\n",
       "  'word',\n",
       "  'hypnotize',\n",
       "  'hypnotize',\n",
       "  'And',\n",
       "  'I',\n",
       "  'love',\n",
       "  'flashy',\n",
       "  'way',\n",
       "  'uh',\n",
       "  'Guess',\n",
       "  'broke',\n",
       "  'paid',\n",
       "  'ha',\n",
       "  'I',\n",
       "  'put',\n",
       "  'hoe',\n",
       "  'NY',\n",
       "  'onto',\n",
       "  'DKNY',\n",
       "  'Miami',\n",
       "  'prefer',\n",
       "  'Versace',\n",
       "  'right',\n",
       "  'All',\n",
       "  'Philly',\n",
       "  'hoe',\n",
       "  'go',\n",
       "  'Moschino',\n",
       "  'come',\n",
       "  'Every',\n",
       "  'cutie',\n",
       "  'booty',\n",
       "  'bought',\n",
       "  'Coogi',\n",
       "  'Now',\n",
       "  'real',\n",
       "  'dookie',\n",
       "  'Meaning',\n",
       "  'really',\n",
       "  'shit',\n",
       "  'Them',\n",
       "  'nigga',\n",
       "  'ride',\n",
       "  'dick',\n",
       "  'Frank',\n",
       "  'White',\n",
       "  'push',\n",
       "  'six',\n",
       "  'Or',\n",
       "  'Lexus',\n",
       "  'LX',\n",
       "  'four',\n",
       "  'half',\n",
       "  'Bulletproof',\n",
       "  'glass',\n",
       "  'tint',\n",
       "  'I',\n",
       "  'want',\n",
       "  'as',\n",
       "  'Gon',\n",
       "  'blast',\n",
       "  'squeeze',\n",
       "  'first',\n",
       "  'ask',\n",
       "  'question',\n",
       "  'last',\n",
       "  'That',\n",
       "  'gangsta',\n",
       "  'pas',\n",
       "  'At',\n",
       "  'last',\n",
       "  'nigga',\n",
       "  'rappin',\n",
       "  'blunts',\n",
       "  'broad',\n",
       "  'Tits',\n",
       "  'bra',\n",
       "  'Ã',\n",
       "  'trois',\n",
       "  'sex',\n",
       "  'expensive',\n",
       "  'car',\n",
       "  'I',\n",
       "  'still',\n",
       "  'leave',\n",
       "  'pavement',\n",
       "  'Condo',\n",
       "  'paid',\n",
       "  'car',\n",
       "  'payment',\n",
       "  'At',\n",
       "  'arraignment',\n",
       "  'note',\n",
       "  'plaintiff',\n",
       "  'Your',\n",
       "  'daughter',\n",
       "  'tied',\n",
       "  'Brooklyn',\n",
       "  'basement',\n",
       "  'Face',\n",
       "  'guilty',\n",
       "  'I',\n",
       "  'stay',\n",
       "  'filthy',\n",
       "  'guilty',\n",
       "  'Richer',\n",
       "  'Richie',\n",
       "  'nigga',\n",
       "  'come',\n",
       "  'get',\n",
       "  'come',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'ca',\n",
       "  'see',\n",
       "  'Sometimes',\n",
       "  'word',\n",
       "  'hypnotize',\n",
       "  'And',\n",
       "  'I',\n",
       "  'love',\n",
       "  'flashy',\n",
       "  'way',\n",
       "  'uh',\n",
       "  'Guess',\n",
       "  'broke',\n",
       "  'paid',\n",
       "  'uh',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'ca',\n",
       "  'see',\n",
       "  'Uh',\n",
       "  'Sometimes',\n",
       "  'word',\n",
       "  'hypnotize',\n",
       "  'hypnotize',\n",
       "  'And',\n",
       "  'I',\n",
       "  'love',\n",
       "  'flashy',\n",
       "  'way',\n",
       "  'uh',\n",
       "  'Guess',\n",
       "  'broke',\n",
       "  'paid',\n",
       "  'uh',\n",
       "  'I',\n",
       "  'fill',\n",
       "  'wit',\n",
       "  'real',\n",
       "  'millionaire',\n",
       "  'shit',\n",
       "  'I',\n",
       "  'fill',\n",
       "  'ya',\n",
       "  'Escargot',\n",
       "  'car',\n",
       "  'go',\n",
       "  'swiftly',\n",
       "  'come',\n",
       "  'Wreck',\n",
       "  'buy',\n",
       "  'new',\n",
       "  'one',\n",
       "  'Your',\n",
       "  'crew',\n",
       "  'crew',\n",
       "  'I',\n",
       "  'know',\n",
       "  'sick',\n",
       "  'name',\n",
       "  'brand',\n",
       "  'nigga',\n",
       "  'Flows',\n",
       "  'girl',\n",
       "  'say',\n",
       "  'sweet',\n",
       "  'like',\n",
       "  'licorice',\n",
       "  'So',\n",
       "  'get',\n",
       "  'nigga',\n",
       "  'easy',\n",
       "  'Girlfriend',\n",
       "  'pen',\n",
       "  'call',\n",
       "  'ten',\n",
       "  'Come',\n",
       "  'sex',\n",
       "  'rug',\n",
       "  'Persian',\n",
       "  'right',\n",
       "  'Come',\n",
       "  'job',\n",
       "  'hit',\n",
       "  'For',\n",
       "  'certain',\n",
       "  'Poppa',\n",
       "  'freakin',\n",
       "  'Leave',\n",
       "  'as',\n",
       "  'leakin',\n",
       "  'like',\n",
       "  'rapper',\n",
       "  'demo',\n",
       "  'Tell',\n",
       "  'Hoe',\n",
       "  'take',\n",
       "  'clothes',\n",
       "  'slowly',\n",
       "  'slowly',\n",
       "  'Hit',\n",
       "  'force',\n",
       "  'like',\n",
       "  'Obi',\n",
       "  'Obi',\n",
       "  'Dick',\n",
       "  'black',\n",
       "  'like',\n",
       "  'Toby',\n",
       "  'Toby',\n",
       "  'Watch',\n",
       "  'roam',\n",
       "  'like',\n",
       "  'Romey',\n",
       "  'Romey',\n",
       "  'Lucky',\n",
       "  'owe',\n",
       "  'Where',\n",
       "  'safe',\n",
       "  'Show',\n",
       "  'homie',\n",
       "  'say',\n",
       "  'homie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'ca',\n",
       "  'see',\n",
       "  'Sometimes',\n",
       "  'word',\n",
       "  'hypnotize',\n",
       "  'And',\n",
       "  'I',\n",
       "  'love',\n",
       "  'flashy',\n",
       "  'way',\n",
       "  'uh',\n",
       "  'Guess',\n",
       "  'broke',\n",
       "  'paid',\n",
       "  'uh',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'ca',\n",
       "  'see',\n",
       "  'Uh',\n",
       "  'Sometimes',\n",
       "  'word',\n",
       "  'hypnotize',\n",
       "  'hypnotize',\n",
       "  'And',\n",
       "  'I',\n",
       "  'love',\n",
       "  'flashy',\n",
       "  'way',\n",
       "  'uh',\n",
       "  'Guess',\n",
       "  'broke',\n",
       "  'paid',\n",
       "  'uh',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'ca',\n",
       "  'see',\n",
       "  'Sometimes',\n",
       "  'word',\n",
       "  'hypnotize',\n",
       "  'And',\n",
       "  'I',\n",
       "  'love',\n",
       "  'flashy',\n",
       "  'way',\n",
       "  'uh',\n",
       "  'Guess',\n",
       "  'broke',\n",
       "  'paid',\n",
       "  'uh',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'ca',\n",
       "  'see',\n",
       "  'Uh',\n",
       "  'Sometimes',\n",
       "  'word',\n",
       "  'hypnotize',\n",
       "  'hypnotize',\n",
       "  'And',\n",
       "  'I',\n",
       "  'love',\n",
       "  'flashy',\n",
       "  'way',\n",
       "  'uh',\n",
       "  'Guess',\n",
       "  'broke',\n",
       "  'paid',\n",
       "  'uh',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'Biggie',\n",
       "  'ca',\n",
       "  'see',\n",
       "  'Sometimes',\n",
       "  'word',\n",
       "  'hypnotize',\n",
       "  'And',\n",
       "  'I',\n",
       "  'love',\n",
       "  'flashy',\n",
       "  'way',\n",
       "  'uh',\n",
       "  'Guess',\n",
       "  'broke',\n",
       "  'paid']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tay = pd.read_csv('lyric.csv', encoding='latin1')\n",
    "tay.columns\n",
    "teste = tay['lyrics']\n",
    "tokenized = [word_tokenize(lyr) for lyr in teste.astype(str)]\n",
    "stop_vec = [[w for w in tok if w not in stop_words] for tok in tokenized]\n",
    "clean_vec = [[word for word in lyr if word.isalpha()] for lyr in stop_vec]\n",
    "wnet = nltk.WordNetLemmatizer()\n",
    "lem = [[wnet.lemmatize(w) for w in lyr] for lyr in clean_vec]\n",
    "\n",
    "lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uh uh uh come Ha sicker yo average Poppa twist cabbage instinct Niggas think shit stink Pink gator Detroit player Timbs hooligan Brooklyn right Dead right head right Biggie night Poppa smooth since day Underoos Never lose never choose bruise crew Do somethin u talk go u come Girls walk u wan na u screw u Who u Yeah Poppa Puff Close like Starsky Hutch stick clutch Dare I squeeze three yo cherry take take take ha ha Bang every MC easily busily Recently nigga frontin ai sayin nothin nothin So I speak piece keep peace come Cubans Jesus piece peep thank Packin askin Who want You got nigga flaunt That Brooklyn bullshit Biggie Biggie Biggie ca see Sometimes word hypnotize And I love flashy way uh Guess broke paid Biggie Biggie Biggie ca see Uh Sometimes word hypnotize hypnotize And I love flashy way uh Guess broke paid ha I put hoe NY onto DKNY Miami prefer Versace right All Philly hoe go Moschino come Every cutie booty bought Coogi Now real dookie Meaning really shit Them nigga ride dick Frank White push six Or Lexus LX four half Bulletproof glass tint I want as Gon blast squeeze first ask question last That gangsta pas At last nigga rappin blunts broad Tits bra Ã trois sex expensive car I still leave pavement Condo paid car payment At arraignment note plaintiff Your daughter tied Brooklyn basement Face guilty I stay filthy guilty Richer Richie nigga come get come Biggie Biggie Biggie ca see Sometimes word hypnotize And I love flashy way uh Guess broke paid uh Biggie Biggie Biggie ca see Uh Sometimes word hypnotize hypnotize And I love flashy way uh Guess broke paid uh I fill wit real millionaire shit I fill ya Escargot car go swiftly come Wreck buy new one Your crew crew I know sick name brand nigga Flows girl say sweet like licorice So get nigga easy Girlfriend pen call ten Come sex rug Persian right Come job hit For certain Poppa freakin Leave as leakin like rapper demo Tell Hoe take clothes slowly slowly Hit force like Obi Obi Dick black like Toby Toby Watch roam like Romey Romey Lucky owe Where safe Show homie say homie Biggie Biggie Biggie ca see Sometimes word hypnotize And I love flashy way uh Guess broke paid uh Biggie Biggie Biggie ca see Uh Sometimes word hypnotize hypnotize And I love flashy way uh Guess broke paid uh Biggie Biggie Biggie ca see Sometimes word hypnotize And I love flashy way uh Guess broke paid uh Biggie Biggie Biggie ca see Uh Sometimes word hypnotize hypnotize And I love flashy way uh Guess broke paid uh Biggie Biggie Biggie ca see Sometimes word hypnotize And I love flashy way uh Guess broke paid']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_tay = [' '.join(lyr) for lyr in lem]\n",
    "single_entry = vectorize.transform(lyrics_tay)\n",
    "s_e = single_entry.todense().tolist()\n",
    "\n",
    "lyrics_tay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictions = {genre: clf.predict(s_e) for genre in genres}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: 0.10%\n",
      "Rock: 2.47%\n",
      "Rap: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "probabilities = {genre: models[genre].predict_proba(s_e)[0][1] for genre in genres}\n",
    "\n",
    "for genre, prob in probabilities.items():\n",
    "    print(f\"{genre}: {prob * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre, model in models.items():\n",
    "    filename = f'{genre}_model.joblib'\n",
    "    joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorize, 'tfidf_vectorizer.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
 